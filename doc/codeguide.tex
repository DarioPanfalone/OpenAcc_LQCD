
%\chapter{Guide to the code}
\newpage
\section*{Preface}

This code can be used for the simulation of LQCD with staggered fermions.
The default choice is for the action is the tree-level Symanzik improved 
action for the gauge part (it is easy to switch back to the standard Wilson one,
setting the correct \#define) and stout-improved fermions (The stout level can be 
set to zero to recover non-stouted fermions).  \\
The implemented algorithm is the Rational Hybrid Monte Carlo. \\
Magnetic fields, imaginary electric fields, imaginary chemical potentials can be 
included in the simulation. This is done by manipulating the $U(1)$ field, which 
also contains the so-called ``staggered phases''.

This guide may have the following problems:
\begin{itemize}
 \item Be outdated
 \item Be wrong
 \item Be somewhat unclear
 \item Contain typos, or bad uses of English 
 \item the ``setting file'' containing the parameters of the simulation
 and other settings can be erratically referred to as ``input file'' 
 (in a simulation there are more input files than one, but there is only one setting file,
 not to be confused with the ``configuration'' file that usually indicates the ``gauge configuration'' file).
\end{itemize}
The author thinks nonetheless that it should be helpful to any user of the code.
In particular, a read of the relevant parts is recommended before trying to use the code,
of asking questions.

If you find any mistake, please report it to the author.

\newpage
\tableofcontents

\newpage
\section{Directory content}

\begin{itemize}
    \item \texttt{src}: the code, for the main program and for some tools.
    \item tools: code and makefile for various utilities, for example to generate the rational approximations and test them and a tool to analyse the performance from information recorded during runs. This directory could also contain tools to convert configurations from a format to another and things like that; 
    \item scripts: basically, collections of commands that you would have to type too many times. 
    At the moment, there are scripts for benchmarking on SLURM clusters, to analyse the performance data produced during simulations,
    and some utilities to test different {\sf tile} configurations.
    \item docs: where documentation should be.
\end{itemize}
\section{Compilation}
\label{compilation}
Compilation is done using Autotools, with the usual {\sf ./configure} , {\sf make} and {\sf make install}.
In case the {\sf configure} script is not present, it can be created from {\sf configure.ac} 
with the command{ \sf autoreconf --install} (notice that this step creates also the {\sf Makefile.in} in the subdirectories, 
using also the information from the files named {\sf Makefile.am}). 
There are a number of parameters that must be passed to {\sf configure} (the choice of the compiler and of the compiler flags), for which the user can find the python script {\sf configure\_wrapper} useful. For example,
\begin{verbatim}
../configure_wrapper pgi cc35 cuda8.0
\end{verbatim}
will produce the command
\begin{verbatim}
../configure CC=pgcc CFLAGS="-acc=noautopar -v -O3 -Minfo=all \
 -ta=tesla:cc35,cuda8.0 -DUSE_MPI_CUDA_AWARE -I${MPIINC}" \
 LDFLAGS="-acc=noautopar -v -O3 -Minfo=all -ta=tesla:cc35,cuda8.0 \
 -lmpi -L${MPILIB}"  --prefix=$(pwd)
\end{verbatim}

\subsection{Generation of the single precision part of the code}
The single-precision part of the code is generated automatically 
by a python script ({\sf src/double\_to\_single\_transformer.py}).

It can be called with {\sf autoMode} to require minimum input. Otherwise, it can act on
single files, or on all files that are known to need the treatment (a list is included in the script).

Notice that there may be some single precision sources or headers which are not just a 
transformation of their double precision counterpart (at the moment, only 
{\sf src/OpenAcc/sp\_fermion\_force.c}). The transformation script contains a list
of these files, which are not touched unless explicitly stated.

{\bf Notice} that this step is performed by {\sf configure}.


\subsection{The actual compilation}

A peculiarity of this code is that, because of design decisions driven by 
performance reasons, the lattice geometry must be konwn at compile time (and 
if you change them, you need to re-run {\sf configure}, run {\sf make clean}, and recompile). 
At \verb|configure| time, it is \emph{necessary} to  specify the dimensions of the local lattice,
the number of ranks in the 4th dimensions (the only one which can be 
parallelized, since we adopted the \emph{salamino} setup), and some optimization parameters
(for the OpenACC  {\sf tile} clause). This information must be written into a 
{\sf geom\_defines.txt} file, an example of which is in the {\sf doc} directory.
The {\sf configure} script will look for that file and read it, setting up the correct makefile.

\subsection{Important notice}

When compiling for a different geometry or different {\sf tile values}, \emph{you need} to execute 
{\sf make clean} first, otherwise only the recently changed files  (if 
any) will be 
compiled with the new geometry, which is different from the one for the rest of 
the files. While the linker may do everything without raising any error, the results will be a 
mess. Clean everything first!\\

For this reason it is advisable to have different directories for different 
geometries.

\subsection{Remarks} 
You may need to do this on the right machine, with the right modules.
For example, on the Pisa localui3, if you use the PGI compiler,
you have to type
\begin{verbatim}
# go on the right machine first:
./scripts/interactive_cudawn.sh 6 1
module load pgi                 # last version
module load cuda                # last version
module load openmpi             # if needed 
\end{verbatim}
Notice the use of the script \verb|scripts/interactive_cudawn.sh|.
On the COKA cluster the modules to load are similar, but there is no need to access
the machines interactively to have access to the compiler.

Notice that the { \bf modules you load in execution must be compatible 
with compilation settings and with the modules loaded during compilation}.
Strange things may happen otherwise.

\section{Run a simulation}

If you want to run a program compiled with PGI, you have to set this 
environment variable in advance.
\begin{verbatim}
export PGI_ACC_BUFFERSIZE= ???? #think
\end{verbatim}
where $????$ stands of a reasonable number, which is larger than the largest 
data transfer you have to perform between the host and the device. The largest 
single data transfer that can be performed is likely to be the gauge 
configuration (it might be different though). \\
{ \bf When running on GPUS, make sure you set \verb|PGI_ACC_BUFFERSIZE| to at 
    least $576$ times the number of lattice sites. Unfortunately, the program WILL 
    NOT TELL YOU when this variable is too small, but the results will be wrong. 
So, be careful.}
A small buffer size will make your program fail (\emph{possibly silently}) 
but a too big one will make your program need too much memory, up to the point 
where it will fail as well.


Once you have set all you need, launch 
{\sf ./main $<$setting\_file$>$}. Here, the program 
will  need some  files that you have to prepare in advance. Another section
of this guide (\ref{filesneeded}) will tell you what you usually need to 
prepare.

Launching \verb|./main| without an input file won't be that useful,
except for the fact that an \verb|input_template| will be generated.
In this repo is also included a \verb|input_openacc| file that serves as
a better template.
You may also want to generate the \emph{rational approximations} in advance. If 
the
program does not find them, il will try to generate them by calling 
the \verb|rgen| executable, which must be present in the \verb|run| directory.

\subsection{Output files}
At present you will have two output files other than standard output (plus an 
optional one), one for the gauge observables and onr for fermionic observables 
(the optional one contains action differences, force moduli and force 
derivative moduli that can be used to tune the parameters in the molecular 
dynamics). To the names of the files a simple hash of the relevant parameters 
in the input files is appended (see \verb|src/Include/hash.c|).\\
Notice that after input file reading during the generation of the 
configurations this hash is displayed.\\
If you want to calculate the hash on an input file, just compile [using make, 
see section (\ref{compilation})] the file \verb|src/Include/checkinput.c|, and 
use it on any input file. The input file reading routines will be called and 
the hash will be displayed again.\\
While this mechanism prevents writing on the same file in two 
different simulations, unfortunately it is very hard to get back from the hash 
(which is part of the file name) to the parameters of the simulation. This 
would require writing a lot more information in the output file names.

\section{Files you may need to run the program}
\label{filesneeded}
The kind of file needed are usually the rational approximations and
the starting gauge configuration file (unless you want to start from
a random configuration).

\subsection{``Setting'' file}
The format of the setting file is pretty self explanatory. If you try
to run \verb|./main| without arguments, it will produce a template, with the 
default values for optional parameters, and comments. You may also cheat, and 
copy from other setting files, but notice that different versions of the program 
may require setting files having different entries.

In general, the setting file can contain comments, escaped with '\#'. It is mad 
up of a number of parameter groups whose beginning is indicated by a parameter 
group name (see below). Each parameter group contains various parameter lines, 
each composed of the parameter name and the parameter value.

The order of the parameter groups in the file is not important, and the order 
of the parameter lines in each group is not important. If a parameter name is 
misspelled, if any unrecognized text is found or if a non-optional parameter 
is not found, the parser will continue to read the whole file, tell you all 
the errors, and terminate the program.

Various remarks on the content of the setting file:

\begin{itemize}
    \item{Theory: \verb|ActionParameters|.}
        Here you can set the value of $\beta$ and the number of stout steps used. You 
        have to write here also the value of $\rho$, but unfortunately at present 
        \verb|RHO| is used in a lot of places in the stouting routines in the code, and 
        the most efficient thing seems to be keeping it as a \verb|#define|: for this 
        reason, only a check is made to see if the value of $\rho$ you want is in 
        the one in code.
    \item{Theory: \verb|FlavourParameters|.}
        If no groups of this kind are found, no fermions will be put in the theory.
        Remark: only purely imaginary chemical potential is allowed (because of the 
        sign problem). 
    \item{Theory: \verb|BackgroundFieldParameters|.}
        An electromagnetic field. Notice that while the magnetic field is real, only 
        a purely imaginary electric field is allowed (because of another sign 
problem). All the parameters here are optional and their default value is zero.
    \item{Simulation, Molecular dynamics and Metropolis test: \verb|MDParameters|.} You can specify the number of steps, 
        the number of gauge substeps for each step, the length of the 
        trajectory, the norm (not squared) of the target residue for matrix 
        inversion during molecular dynamics evolution, if you want to use 
        the single precision version, and the target residue for matrix 
inversion in the pre-MD and post-MD operations (generation of the $\chi$ 
fermions and final action computation respectively). While the other two 
parameters should be kept as  
        low as possible to optimize simulation time, the length of the trajectory 
        should be kept equal to $1$ but for bugfixing purposes.

\item{Properties of the run: \verb|MontecarloParameters|.} A lot of parameters here: 
\begin{itemize}
\item \verb|NTraj| is the number of trajectories that have to be performed in this run.
\item \verb|ThermNTraj| is the number of trajectories from the beginning of the 
\emph{history} for which the Metropolis test can be skipped. Notice that, if the 
run starts from a configuration which has a nonzero \verb|conf_id| (because,
for example, it comes from an earlier run) and if \verb|conf_id| is larger than 
\verb|ThermNTraj|, the Metropolis test won't be skipped at all.
\item \verb|StoreConfInterval|: configurations will be stored, with 
\verb|conf_id| in their name, once every this number of trajectories. The name is will
be the one specified by \verb|StoreConfName| with \verb|conf_id| attached to it. 
\item \verb|SaveConfInterval|: the file specified in \verb|SaveConfName| will be saved once every this number of trajectories. For safety, set 1, for speed, you can increase it.

\item Thing related to the initialization of the random number generator. \verb|RandGenStatusFilename| is 
used to specify the name of a file where the random number generator status 
will be stored. At present, it is optional and the default value is 
\verb|rgstatus.bin|. If this file is not found, the variable \verb|Seed| is 
instead use to initialise the random number generator. If \verb|Seed| is zero, 
the function \verb|time()| will be used. Notice that after a successful run 
the file will be generated, and will be read on the next run, while \verb|Seed| 
will be ignored. Notice also that the name of the file depends on the MPI rank when more than one MPI 
process is used.
\item \verb|JarzynskiMode|: setting this to 1 or -1 will turn on Jarzynski 
mode. By default, this option is 
inactive, and the parameter is set to 0 (see Sec. \ref{JarzynskiMode}).
\end{itemize}
You can also set the \verb|Seed| of the simulation, a parameter 
\verb|EpsGen| defining the randomness in cold gauge conf generation, and the 
verbosity level \verb|VerbosityLv|.



\item{Measures: \verb|GaugeMeasuresSettings|.} Now, only the name of the file
for the {\bf basic} gauge measures is specified (plaquette, rectangle, real and imaginary part of the Polyakov loop). Notice that the name of the output 
file will be dependent on the relevant parameters of the input file, through a
simple hash procedure (see \verb|src/Include/hash.c|.). Of course there are, in principle, a lot of observables that can be implemented. Now only the Polyakov
loop and the topological charge are implemented but the topological charge 
measurement (which involves cooling) is not currently handled in the input file. 
\item{Measures: \verb|FermionMeasuresSettings|.}
 Apart from the file name (which will be modified as in the gauge case),   
you can set how many random vectors will be used for the measurement of various 
fermionic quantities. For the chiral condensate and the quark number only one 
inversion for random vector is needed. For the susceptiiblities, two inversions 
per random vector are needed. The inversion in the chiral case and the inversion 
in the quark number case have to be performed on different vectors. Notice: the 
number of ``single inversions'' must be higher than the number of ``double 
inversions''; this is because each ``double inversion'' 
contains a ``single inversion'', so it is assumed that there are no reasons to 
ask for \verb|SingleInvNVectors| to be smaller than \verb|DoubleInvNVectorsChiral|
or \verb|DoubleInvNVectorsQuarkNumber|.
    
\item{Device: \verb|DeviceSettings|.}  Here you can set the number of 
mpi ranks (one or more), which must be equal to  the one set at compilation 
time. Rank 0 (or the whole program, in the case of a single process) will run 
on the device speficied by \verb|device_choice|. The other ranks, will chose 
different gpus according to the scheme
\begin{verbatim}
 d = (device_choice + mpi_rank) % NProcPerNode
\end{verbatim}
\textbf{Notice: } set this to 0 when using slurm as a job scheduler. In any 
case, ask the sysadmin about this.\\


\item{\verb|Geometry|} Here the lattice dimensions are listed and the 
direction mapping too. Unfortunately, the lattice dimensions cannot be set in 
this way; as in the case of \verb|StoutRho| they are read just to check the 
intention of the user and suggest recompilation. \\
Note that only one dimension can be parallelized, and for performance 
reasons it must be the one relative to the most extern index in the arrays, 
that is the fourth one. With the ``map'' parameters, you can choose which 
physical directions correspond to each logical direction. By default, 
the map is $(0,1,2,3) = (x,y,z,t)$.

\item{Algorithmic optimizations: \verb|InverterTicks|. }
 These actually refer to the single shift inverter: very little is doable for 
the multishift one.
\begin{itemize}
 \item \verb|singlePInvAccelMultiInv| : don't touch this, multishift 
inverter is still faster (keep 0 or delete).
 \item \verb|useMixedPrecision|: in single inversions, this should provide a 
substantial boost. Default is 0, by now.
 \item \verb|restartingEvery| : restarting of the inversion algorithm takes 
place every how-many-you-decide steps. Default 10000.
 \item \verb|mixedPrecisionDelta| : in the mixed precision CG algorithm, the 
residue must be calculated in double precision at some points. In practice, it 
is recalculated in double precision when its magnitude varies of a factor, 
which is specified here. Default : 0.1.
\end{itemize}
        
\item{Debug: \verb|DebugSettings|.}
\begin{itemize}
 \item \verb|UseILDG| : if this flag is set to $1$, the gauge configuration 
file  I/O will be done using ILDG format. The default value is 1, so ILDG 
activated.
 \item \verb|SaveAllAtEnd| : In case you want to reproduce a bug, you want to 
start from the same gauge configuration and from the same state of the random 
number generator(s). Setting this option to 0 will make the program not 
overwrite the gauge configuration file and the random generator status file(s). 
The default value is 1, of course.
 \item \verb|SaveDiagnostics| : Flag to save information about performance (time measurements, number of Dirac operator iterations, action differences, gauge and
 fermion forces measurements, gauge and fermion forces differences across a step, acceptance...)
 \item \verb|DoRevTest| : Flag to run the reversibility test.
 \item \verb|DoNoRandomTest| : Enable a mode where no random numbers are used.
 \item \verb|RngFakenessLevel| : If set to zero, normal pseudorandom numbers are used. 
 If set to 1, in the initialization of fermions and momenta field is done taking using numbers that depend on the physical location, and once the initialization is finished, always the same number is returned (to know more for the variable {\sf rng\_fake\_gl\_index }). If set to 2, the random number generator always outputs $0.5$.
 \item \verb|MDDbgPrintMaxCount| : The number of times various quantities are printed to files, like fermions, staples etc. This can be set to a number greater than zero to do a low-level check of the effect of a code change in molecular dynamics, and should allow to pinpoin the problem.
 \item \verb|SaveDiagnosticsFileName| : the name of the file where the performance measurement activated with the flag \verb|SaveDiagnostics| are saved. Notice that in {\sf scripts} there is a shell script, {\sf analyse\_md\_info.sh }, which can digest the data printed in this file (of course, if the diagnostics file format is changed, the analysis script may need to be updated).
 \item \verb|PrintDiagInfoEvery| : specifies how frequently the performance\&diagnostics
 measurements to save in the file have to be taken. Notice that measurements for the gauge forces will be taken {\sf GaugeSubSteps} times more often than the measurements for the fermion force, except in a multi-node setup.
\end{itemize}

\item{Other Test setting, for benchmarking: \verb|TestSettings|.}
\begin{itemize}
\item \verb|DeoDoeIterations| : How many iterations to do during the Dirac operator benchmark.
\item \verb|MultishiftInverterRepetitions| : How many repetitions of the inversion to do
during the Multi Shift inverter benchmark.
\item \verb|FakeShift| : In the MultiShift inverter test, if \verb|BenchmarkMode| is set, a fake rational approximation is used. This sets the value of the shift, for all terms.
\item \verb|BenchmarkMode| : Flag to tell the MultiShift inverter benchmark to use a fake ration approximation.
\item \verb|SaveResults| : Flag to make so that the results of the multiplication by the Dirac operator (and its intermediate operations) are saved to a file, for comparison. 


\end{itemize}
\end{itemize}


\subsubsection{Remarks: verbosity level}
Don't forget to set the \verb|VerbosityLv| variable to the desired result. 
The value of it is assigned to the \verb|verbosity_lv| global variable.
As a rule of thumb, '5' prints all, '1' prints nearly nothing. Note
that the behaviour of \verb|VerbosityLv| is likely to change over time, no
policy is yet decided. If you implement anything new, you can use this 
feature to give informative messages.

\subsubsection{Remarks: Optimization of parameters}

There is an interplay between \verb|NmdSteps|, \verb|GaugeSubSteps|,
\verb|residue_md| and the order of the rational approximations used in the 
molecular dynamics. The goal of the optimization is to obtain a reasonable 
value of the acceptance from the Metropolis test while using the lowest value 
possible. For a discussion on this topic, see Sec.(\ref{tuning}).

\subsubsection{Remarks: seemingly insignificant things that you have to take 
care of}
A parameter named \verb|ExpMaxEigenvalue| can be used in the 
\verb|MontecarloParameters| group, and represent an estimate
of the maximum eigenvalue of the Dirac Operator. Its default value is $5.5$.  
This will influence  the choice of the rational approximations you need. 
Notice that the highest eigenvalue of the fermion matrix is calculated during 
the update and printed out if the right verbosity value is set. Notice that the 
smaller \verb|StoutSteps is|, the larger this number is likely to be. 

What you \emph{can't} set with the input file (now) and you have to
recompile\footnote{ It is not a problem if you use gcc or pgi to compile for 
    CPUs, but compiling with pgi for GPUs can require time, depending on the
version of the compiler.}:
\begin{itemize}
    \item The type of gauge action. [triggers some long recompilation]
    \item The stouting strength RHO 
    \item The lattice dimensions: they must be known at {\sf configure} time.
\end{itemize}
Of course, changing the input file format requires to recompile the functions 
to read it, in \verb|/src/Include/init.c|.

\subsection{ Rational Approximation files}
Rational approximations are used to approximate powers (like $1/4$,
$1/8$) of the Dirac Operator.
You need $3$ rational approximations files for each quark flavour in the
theory, one for the first inversion (to map the original
gaussianly generated pseudofermion to one generated according to the
right pdf), one used during the molecular dynamics evolution (with a
lower order, so less precise, for speed), and a third one for the
calculation of the final action for the metropolis test (more
precise). At the time of writing, it is common that at least 2 flavour are 
degenerate in mass, and this allows you to use the same set of rational
approximations. For example, for a $N_f=2+1$ simulation, you need a total
of 6 rational approximations.
NOTICE: the name of the rational approximation file is related to
its content, and can be generated using the function
\verb|char* rational_approx_filename()| in
\verb|src/RationalApprox/rationalapprox.h|.

\subsubsection{ How to generate and check rational approximations}
The sources of the tools are, guess what, in  the \verb|tools| directory.
Notice that the program \verb|rgen| is used semi-automatically to generate the 
rational approximation files. If the motecarlo program runs on a single MPI rank, 
if a rational approximation file is not found it is created by an external call to \verb|rgen|.
If the motecarlo program runs instead on multiple MPI ranks, \verb|rgen| will not be called: 
instead, a bash script will be created names {\sf genappfiles.sh}, containing all the necessary commands (involving \verb|rgen|)
to create and cache the rational approximation files. Notice that the commands 
will be repeated {\sf nranrk} times. The wise way to use this script is 
\begin{verbatim}
 bash <(sort genappfiles.sh | uniq)
\end{verbatim}
so to remove duplicates.

A brief description of the rational approximation tools follows:
\begin{itemize}
    \item '\verb|rgen|' (\verb|rationalapprox_calc.c|) 
        the tool for generating them, it takes the tolerance (plus other
        things) as input and will try to increase the number of poles until
        the required level of acuracy is reached. It takes time to generate
        high order approximations (~minutes). Notice that it needs arbitary
        precision libraries to be compiled.
    \item '\verb|rgenfo|' (\verb|rationalapprox_calc.c|) 
        Same as rgen, but the order of the approximation is fixed.
    \item '\verb|ratnormalizer|' (\verb|normalize_ratapprox.c|) 
        takes a rational approx and
        rescales it so that the upper limit of the validity range is 1.
    \item '\verb|eval_ratapprox|' (\verb|eval_ratapprox.c|)
        Are you sure that your rational approx works? With this tool, you
        can check! \verb|bc -l| is your friend too.
\end{itemize}

\subsection{Gauge configuration files}
For the format of the gauge configuration files, look in 
\verb|src/OpenAcc/io.c|  for functions named \verb|print| or \verb|read|, or similar.
Notice there are two possible formats for the gauge configuration files: the 
ASCII format, and the ILDG format. The first one is simple and is useful for 
simplicity. The ILDG file format is a standard format used in Lattice QCD, 
where the data is stored in binary format (notice: big endian).  
The read and/or write of the initial/final gauge configuration file
is usually done in \verb|/src/OpenAcc/main.c|.


\section{Debug Tools}
Given the intended usage of debug tools, the user/developer should be always 
wary of them, and check that they work as he would expect: debug tools are not 
tested as extensively as the production code. None the less, it seems pretty
reasonable to add some work that has been done on this tools in the released 
code.
\subsection{ No random Mode}
If you need your run to be reproducible for a whole molecular dynamics 
trajectory, you don't trust the random number generatorand don't want to use 
it, you can set {\sf DoNoRandomTest } to 1. It may also be useful to shorten the length of the trajectory (to improve acceptance).
At present random number generation is used in:
\begin{itemize}
    \item The generation of a new configuration;
    \item The generation of the pseudo fermions for the molecular dynamics 
        evolution;
    \item The generation of the momenta for the molecular dynamics evolution.
    \item It is also used to generate the pseudofermions in stochastic fermion 
        measurements, however this is our of the scope of ``no random mode''.
\end{itemize}
The only files where this flag has an effect are, at present, 
\verb|/src/OpenAcc/main.c| and \verb|/src/OpenAcc/update_versatile.c|. 


\subsection{The ``test\_and\_benchmarks'' directory}
In this directory you can find the sources for the most important benchmarks:
\begin{itemize}
    \item Tests and benchmarks for $D_{eo}$ and $D_{oe}$, the Dirac 
        operator (\verb|deo_doe_test.c|), in single an double precision.
    \item Test and benchmarks for the multishift inverter in single and double 
precision (\verb|inverter_multishift_test.c|), in single and double precision. 
\end{itemize}

\subsection{The DbgTools directory}
In this directory you can find the sources for some low-level tests
which are not maintained, but can hopefully be useful - either the code can be easily 
adapted for your needs, or you can take inspiration from that. 
In the header file \verb|dbgtools.h| you can find the declarations of many 
functions that print out or read from file various datatypes, which instead can be expected 
to work (for example, they are used in the \verb|deo_doe_test| benchmark).

\section{Tuning}
\label{tuning}

\subsection{Low-level optimization with the {\sf tile} OpenACC clause}

At compile time it is possible to set the values of the {\sf tile } and {\sf gang} sizes 
for the 4 most significant kernels - the ones pertaining to the 
dirac operator, to the improved staples, to the normal staples, and to the computation of
$\Sigma$ (an ingredient used during stouting).
Finding the optimal values for the {\sf tile} and {\sf gang} clauses requires testing. 
In the {\sf scripts} directory there are two scripts, namely 
\verb|test_blocks_dirac_operator.sh| and 
\verb|test_blocks_puregauge.sh|, which read two data files - 
\verb|dirac_blocks.txt| and 
\verb|puregauge_blocks.txt| respectively - and compile different version of the benchmarks, 
so that they can be tested. Examples of the \verb|dirac_blocks.txt| and 
\verb|puregauge_blocks.txt| can be found in the {\sf scripts} directory. 

A possible use of these scripts is the following.
\begin{enumerate}
 \item Create a \verb|build| directory, build the program there (configure, make and 
 make install, see before)
 \item Create the \verb|dirac_blocks.txt| and/or \verb|puregauge_blocks.txt|, writing 
 the blocks you want to test inside it
 \item Run the command
 \begin{verbatim}
../scripts/test_blocks_dirac_operator.sh
 \end{verbatim}
 and/or
 \begin{verbatim}
../scripts/test_blocks_puregauge.sh  
 \end{verbatim}
\end{enumerate}
 These scripts will create different versions of the benchmark executables. In order to run 
 them, you may want to insert the executables in a suitable slurm or LSF script.
 A tool to create slurm scripts is described in Chapter \ref{tests_and_benchmarks}.
 
\subsection{Molecular dynamics parameters}
Holy texts report that optimal hybrid montecarlo acceptance should be between 70\% and 80\%. 
The probability for a new configuration to be accepted is related to the action 
difference between the start and the end of the trajectory, which should be zero 
in perfect conditions. The reasons why it \emph{will} be different from zero are:
\begin{itemize}
    \item A mismatch between the 3 rational approximations used. A fast test could 
        be done to check if the first one and the last one are in good agreement, just removing all molecular dynamics in between
        %\footnote{At the moment, this result is NOT achieved by putting the relevant %parameters in the input file to zero.}. 
        % CHE MINCHIA VOLEVO DIRE?
    \item Errors in the integration of the trajectory.
\end{itemize}
As far as the first point is concerned: since we want our results to be 
accurate, we would like the last inversion to be as 
``exact'' as possible, while we can save some computing time on the molecular dynamics 
inversions, which will be performed many times and whose precision (as long as the last 
action calculation is ``exact'') can be traded for speed. The first inversion can be 
done precisely since it will be performed only once. Optimization can thus be made with
the choice of the rational approximations used during molecular dynamics. There is a 
single parameter which must be tuned, which is the maximum error of the approximation. 
Reducing the error will determine an increase in the order of the approximation and a 
reduction in the target residue in the inversions. The higher the order, the slower; 
the smaller the target residue, the slower. \\
As far as the integration errors are concerned, the most basic observation is that, for 
the Omelyan 2nd order integrator, the action difference (which is an error) should go 
like $[t/n_{md}]^2$, where $n_{md}$ is the number of steps in the molecular dynamics 
trajectory, while $t$ is the molecular dynamics time
\footnote{(Note for self) 
this topic has been already discussed in the literature, check}. 
While there is no way a priori to fix the parameters apart from 
experience (read: look to another successful run), once a choice of the 
parameters is reached for which acceptance is satisfying, there are some 
possible rules of thumb that may help in optimizing $n_{md}$ and the number 
of gauge substeps $n_{gss}$:
\begin{itemize}
    \item The norm of the fermion force times the number of gauge substeps should 
        be 
        roughly equal to the norm of the gauge force.
    \item The norm of the variation of the fermion force every half Omelyan macro 
        step 
        (which is $n_{gss}$ times a micro step), and the norm of the variation of the 
        gauge 
        force every Omelyan micro step should be equal.
    \item (Not implemented) The same reasoning could be done with the maximum values 
        found.
\end{itemize}

Notice that informations about these quantities can be obtained by setting the appropriate flags in the setting file, in the section {\sf DebugSettings}, and the 
data produced can be analysed with the {\sf analyse\_md\_info.sh} script.







\section{Setting file reading (and advices for format modifications).}
Setting file reading is done in \verb|/src/Include/setting_file_parser.c|. The idea here is to 
read parameters from a file and store them into global variables (or, better, 
into variables of limited number of global structures). The input file should 
be composed by parameter ``macro groups'', with a name that is needed to find 
them, and each of the parameter macro groups consist of a collection of tags 
which are put next to a value (an integer, a floating point number, or a 
string). The function 
\verb|set_global_vars_and_fermions_from_input_file()| does the 
following things:
\begin{enumerate}
    \item Tries to open the specified input file. If it is successful:
        \begin{enumerate}
            \item Loads all the file as an array of lines, and modifies lines removing 
                comments (everything beyond a \verb|#| sign, including \verb|#| itself)
            \item Scan the file to find names of macro-groups of parameters, thus 
                identifying for each group a type, an initial line and an ending line;
            \item For each of the groups found, it calls the appropriate parser (which 
                is actually a wrapper) passing to it the correct \emph{global} variable (which is a structure), 
                the start file line and the end file line of the group. Every parser-wrapper 
                defines an array of structures of the kind \verb|par_info|:
\begin{verbatim}
//types that can be found in an input file
enum dtype {TYPE_INT, TYPE_DOUBLE, TYPE_STR, NUM_TYPES};
const char * type_strings[]={"(int)", "(double)", "(string)" };

typedef struct par_info_t{

    void* par;
    enum dtype type;
    const char* name;
    const void* default_value;
    const char* comment;

}par_info;
\end{verbatim}
                Inside every parser-wrapper, this array is passed to a real parser that looks 
                for the \verb|name| in the range of lines specified by the parser-wrapper, 
                reads a value whose type is speficied in \verb|type|, and puts it into the 
                variable pointed at by \verb|*par|. If no value specified by \verb|type| is 
                found, the \verb|*default_value| is used instead, and a message is printed.
                If no value is read and  \verb|defalt_value| is \verb|NULL|, an error will be generated,
                but the read of the setting file will continue until the end in order to 
                give print all the errors.
                
        \end{enumerate}
    \item If it is not opened for any reason, it goes into ``help'' mode, and the 
        same chain of parser-wrappers and parser is called in help mode writing an 
        \verb|input_template| file.
\end{enumerate}
If you want to add another parameter ``macro group'', you have to add the name 
to the \verb|par_macro_groups_names| list, increasing also \verb|NPMGTYPES|.
You also need to create the correct parser-wrapper for it, and modify the \\
\verb|set_global_vars_and_fermions_from_input_file()| \\function to accomodate 
the reading of that parameter ``macro group''.\\
If you want just to add a parameter to an existing group, you just have to 
modify its parser-wrapper and possibly the structure that should hold the value 
of that new parameter.

\section{Gauge Configuration file reading}
All this is performed in \verb|src/OpenAcc/io.c|.
Notice that in the case there are more than one MPI process the gauge configuration
file is read by rank 0, and then split into different pieces which are sent to the 
other ranks. The reverse happens when the gauge configuration is saved to disc.

\subsection{ASCII Format}
The ASCII format is really useful for checking and debugging, and at present is 
the only one which is extensively tested. This format is fast to modif (of 
course you need to modify both reading and writing functions), and being able 
to edit the configuration files with a text editor might be useful.
Unfortunately files tend to be files which are 3 times larger than binary 
files, and for large lattices this might be a problem.

\subsection{ILDG format (binary)}
By activating the correct flag in the input file, the ILDG format will be read 
and written instead. The ILDG format is used for gauge configuration 
storage which is a diffused standard (Google it). It is made up by LIME records 
(google LIME, scidac, there is also a GitHub repo with documentation), which 
are made up of a header and a body. Binary data in the header and the body are 
assumed to be Big Endian. The header has a ``type'' string, which identifies 
the 
kind of record.
The standard ILDG format requires a \texttt{ildg-format} record, an 
\texttt{ildg-binary-data} record, and a \texttt{ildg-data-lfn} record:
\begin{itemize}
    \item The \texttt{ildg-format} record contains, most notably the dimensions of 
        the configuration, the precision and the kind of gauge field (in ASCII);
    \item \texttt{ildg-binary-data} contains instead the values of the 
        configuration gauge links in binary format;
    \item The \texttt{ildg-data-lfn} is not actually used here.
\end{itemize}
Non standard records:
\begin{itemize}
    \item \verb|MD_traj|: a number identifying the Montecarlo Trajectory, needed 
        to restart the simulation (ASCII format).
    \item \verb|input-file|: the entire input file with comments (ASCII format).
\end{itemize}
You can look into \verb|src/OpenAcc/io.c| or into a gauge configuration 
file with a text editor to get more informations.

\subsection{Configuration file format conversion}
A tool for file conversion is \verb|/src/DbgTools/ildgtest.c|. You can add it 
to the list of arguments you pass to \verb|generate_makefile.py| in order to 
build a makefile that will have, amongst other targets,\verb|ildgtest|.
Modify the \verb|main()| to obtain your desired result.

\section{External Fields, staggered phases and field-dependent observables}

The gauge links {\bf must never} be stored in a state where they
are multiplicated by the staggered phase. All the gauge links are just
plain SU(3) matrices.

This has some consequences. The staggered phases are always passed in the "external field".
The external field also incorporates the effect of a static abelian
field and the effect of a nonzero chemical potential.
This means we need a different external field for each of the
flavours.

This necessity is propagated also in measurements of observables.

Notice that most observables concerning the external field - magnetizations, 
quark numbers, and susceptibilities - contain terms that come from the derivative 
of the links w.r.t. the external field. Since usually the $U(1)$ phases are proportional 
to the external field, in order to build the observable the easiest thing is to 
compute the phases for two values of the field and then take the difference.
This may be particularly convenient when considering that the mapping of lattice axes to physical axes 
must be totally free.

Note: at present, only the magnetization is implemented using this trick. The 
quark number and quark number susceptibilities are instead stupidly implemented, requiring e.g. 4 different 
implementations for the 4 possible orientations of the $\tau$ axis.

\section{Jarzynski Mode}
\label{JarzynskiMode}

If the flag \verb|JarzynskiMode| is set to 1 (or -1) the value of the $z$-component of the magnetic field is incremented (decremented)
by a little step after every molecular dynamics trajectory. It goes from an integer to the next (or the previous, if \verb|JarzynskiMode| is set to -1)
in a number of steps that is specified by \verb|MaxConfIdIter|. 
The number of measurements of fermionic quantities is set in the \verb|FermionMeasuresSettings| group.


