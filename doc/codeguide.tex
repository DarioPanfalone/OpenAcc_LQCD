
\chapter{Guide to the code}

\section{Directory content}

\begin{itemize}
    \item \texttt{src}: the code, for the main program and for some tools.
    \item tools: code and makefile for various utilities, for example to generate the rational approximations and test them and a tool to analyse the performance from information recorded during runs. Tthis directory could also contain tools to convert configurations from a format to another and things like that; 
    \item scripts: basically, collections of commands that you would have to type too many times. 
    At the moment, here there are only scripts for benchmarking on SLURM clusters.
    \item docs: where documentation should be.
\end{itemize}
\section{Compilation}
\label{compilation}
Compilation is done using Autotools, with the usual {\sf ./configure} , {\sf make} and {\sf make install}.
In case the 
There are a number of parameters that must be set (the choice of the compiler and of the compiler flags), for which the user can find the python script {\sf configure\_wrapper} useful. For example,

\begin{verbatim}
../configure_wrapper pgi cc35 cuda8.0
\end{verbatim}
will produce the command
\begin{verbatim}
./configure CC=pgcc CFLAGS="-acc=noautopar -v -O3 -Minfo=all \
 -ta=tesla:cc35,cuda8.0 -DUSE_MPI_CUDA_AWARE -I${MPIINC}" \
 LDFLAGS="-acc=noautopar -v -O3 -Minfo=all -ta=tesla:cc35,cuda8.0 \
 -lmpi -L${MPILIB}"  --prefix=$(pwd)
\end{verbatim}


\subsection{Generating the single precision part of the code}
The single-precision part of the code is generated automatically 
by a python script ({\sf src/double\_to\_single\_transformer.py}).

It can be called with {\sf autoMode} to require minimum input. Otherwise, it can act on
single files, or on all files that are known to need the treatment (a list is included in the script).

Notice that there may be some single precision sources or headers which are not just a 
transformation of their double precision counterpart (at the moment, only 
{\sf src/OpenAcc/sp_fermion_force.c}). The transformation script contains a list
of these files, which are not touched unless explicitly stated.


\subsection{Creating the makefile(s)}

This is managed by autotools.

\subsection{The actual compilation}

A peculiarity of this code is that, because of design decisions driven by 
performance reasons, the lattice geometry must be konwn at compile time (and 
if you change them, you need to \verb|make clean| and recompile). At \verb|make|
time, it is \emph{necessary} to  specify the dimensions of the local lattice, 
and the number of ranks in the 4th dimensions (the only one which can be 
parallelized, since we adopted the \emph{salamino} setup). It is also 
\verb|nice| to specify the hash of the commit of the code, so that any 
executable produced will show it and you will be able to understand which 
version of the source code was used. Example:
\begin{verbatim}
make N0=32 N1=32 N2=32 N3=8 NR3=4 COMMIT_HASH=$(git rev-parse HEAD) main
\end{verbatim}
Or, if you want to make all necessary things:
\begin{verbatim}
make N0=32 N1=32 N2=32 N3=8 NR3=4 COMMIT_HASH=$(git rev-parse HEAD) 
\end{verbatim}
this will \emph{make} all the ``main'' files you passed to 
\verb|generate_makefile.py|, plus \verb|rgen|, the executable needed to 
generate the files of the rational approximations (this is because the first 
target in the makefile, which is \verb|all|, includes all the files you 
specified when creating the makefile in the first place).\\
Notice that the \verb|make| process, if started from scratch, can take several 
minutes (with some versions of the PGI compiler, up to about one hour!).

\subsection{Important notice}

When compiling for another geometry, \emph{you need} to execute 
\verb|make clean| first, otherwise only the recently changed files  (if 
any) will be 
compiled with the new geometry, which is different from the one for the rest of 
the files. While 
the linker may to everything without raising any error, the results will be a 
mess. Clean everything first!\\

For this reason it is advisable to have different directories for different 
geometries.

\subsection{Remarks} 
You may need to do this on the right machine, with the right modules.
For example, on the Pisa localui3, if you use the PGI compiler,
you have to type
\begin{verbatim}
../scripts/vai_su_cudawn?.sh    # choose 6 or 7
module load pgi                 # last version
module load cuda                # last version
module load openmpi             # if needed 
\end{verbatim}




FAQ \\
Q: Why don't you just use autotools? \\
A: Because I don't know how to use them! If you can do better, do
it (please).



\section{Run a simulation}

If you want to run a program compiled with PGI, you have to set this 
environment variable in advance.
\begin{verbatim}
export PGI_ACC_BUFFERSIZE= ???? #think
\end{verbatim}
where $????$ stands of a reasonable number, which is larger than the largest 
data transfer you have to perform between the host and the device. The largest 
single data transfer that can be performed is likely to be the gauge 
configuration (it might be different though). \\
{ \bf When running on GPUS, make sure you set \verb|PGI_ACC_BUFFERSIZE| to at 
    least $576$ times the number of lattice sites. Unfortunately, the program WILL 
    NOT TELL YOU when this variable is too small, but the results will be wrong. 
So, be careful.}
A small buffer size will make your program fail (\emph{possibly silently}) 
but a too big one will make your program need too much memory, up to the point 
where it will fail as well.


Once you have set all you need, go into \verb|./run| and launch 
\verb|./main <input_file>|. Here, the program 
will  need some  files that you have to prepare in advance. Another section
of this guide (\ref{filesneeded}) will tell you what you usually need to 
prepare.

Launching \verb|./main| without an input file won't be that useful,
except for the fact that an \verb|input_template| will be generated.
In this repo is also included a \verb|input_openacc| file that serves as
a better template.
You may also want to generate the \emph{rational approximations} in advance. If 
the
program does not find them, il will try to generate them by calling 
the \verb|rgen| executable, which must be present in the \verb|run| directory.

\subsection{Output files}
At present you will have two output files other than standard output (plus an 
optional one), one for the gauge observables and onr for fermionic observables 
(the optional one contains action differences, force moduli and force 
derivative moduli that can be used to tune the parameters in the molecular 
dynamics). To the names of the files a simple hash of the relevant parameters 
in the input files is appended (see \verb|src/Include/hash.c|).\\
Notice that after input file reading during the generation of the 
configurations this hash is displayed.\\
If you want to calculate the hash on an input file, just compile [using make, 
see section (\ref{compilation})] the file \verb|src/Include/checkinput.c|, and 
use it on any input file. The input file reading routines will be called and 
the hash will be displayed again.\\
While this mechanism prevents writing on the same file in two 
different simulations, unfortunately it is very hard to get back from the hash 
(which is part of the file name) to the parameters of the simulation. This 
would require writing a lot more information in the output file names.

\section{Files you may need to run the program}
\label{filesneeded}
The kind of file needed are usually the rational approximations and
the starting gauge configuration file (unless you want to start from
a random configuration).

\subsection{``Setting'' file}
The format of the setting file is pretty self explanatory. If you try
to run \verb|./main| without arguments, it will produce a template, with the 
default values for optional parameters, and comments. You may also cheat, and 
copy from other setting files, but notice that different versions of the program 
may require setting files having different entries.

In general, the setting file can contain comments, escaped with '\#'. It is mad 
up of a number of parameter groups whose beginning is indicated by a parameter 
group name (see below). Each parameter group contains various parameter lines, 
each composed of the parameter name and the parameter value.

The order of the parameter groups in the file is not important, and the order 
of the parameter lines in each group is not important. If a parameter name is 
misspelled, if any unrecognized text is found or if a non-optional parameter 
is not found, the parser will continue to read the whole file, tell you all 
the errors, and terminate the program.

Various remarks on the content of the setting file:

\begin{itemize}
    \item{Theory: \verb|ActionParameters|.}
        Here you can set the value of $\beta$ and the number of stout steps used. You 
        have to write here also the value of $\rho$, but unfortunately at present 
        \verb|RHO| is used in a lot of places in the stouting routines in the code, and 
        the most efficient thing seems to be keeping it as a \verb|#define|: for this 
        reason, only a check is made to see if the value of $\rho$ you want is in 
        the one in code.
    \item{Theory: \verb|FlavourParameters|.}
        If no groups of this kind are found, no fermions will be put in the theory.
        Remark: only purely imaginary chemical potential is allowed (because of the 
        sign problem). 
    \item{Theory: \verb|BackgroundFieldParameters|.}
        An electromagnetic field. Notice that while the magnetic field is real, only 
        a purely imaginary electric field is allowed (because of another sign 
problem). All the parameters here are optional and their default value is zero.
    \item{Simulation: \verb|MDParameters|.} You can specify the number of steps, 
        the number of gauge substeps for each step, the length of the 
        trajectory, the norm (not squared) of the target residue for matrix 
        inversion during molecular dynamics evolution, if you want to use 
        the single precision version, and the target residue for matrix 
inversion in the pre-MD and post-MD operations (generation of the $\chi$ 
fermions and final action computation respectively). While the other two 
parameters should be kept as 
        low as possible to optimize simulation time, the length of the trajectory 
        should be kept equal to $1$ but for bugfixing purposes.

\item{Simulation: \verb|MontecarloParameters|.} A lot of parameters here: 
\begin{itemize}
\item \verb|NTraj| is the number of trajectories that have to be performed in 
    this run.
\item \verb|ThermNTraj| is the number of trajectories from the beginning of the 
    \emph{history} for which the Metropolis test can be skipped. Notice that, 
if 
    the run starts from a configuration which has a nonzero \verb|conf_id| 
    (because, for example, it comes from an earlier run) and if \verb|conf_id| 
is 
    larger than \verb|ThermNTraj|, the Metropolis test won't be skipped at all.
\item \verb|StoreConfInterval|: configurations will be stored, with 
    \verb|conf_id| in their name, once every this number of trajectories. The 
name 
    is will be the one specified by \verb|StoreConfName| with \verb|conf_id| 
    attached to it. 
\item \verb|SaveConfInterval|: the file specified in \verb|SaveConfName| will 
    be saved once every this number of trajectories. For safety, set 1, for 
speed, 
    you can increase it.

\item Thing related to random initialization. \verb|RandGenStatusFilename| is 
used to specify the name of a file where the random number generator status 
will be stored. At present, it is optional and the default value is 
\verb|rgstatus.bin|. If this file is not found, the variable \verb|Seed| is 
instead use to initialise the random number generator. If \verb|Seed| is zero, 
the function \verb|time()| will be used. Notice that after a successful run 
the file will be generated, and will be read on the next run, while \verb|Seed| 
will be ignored.
\item \verb|JarzynskiMode|: setting this to 1 or -1 will turn on Jarzynski 
mode. This is an experimental feature (see below). By default, this option is 
inactive, and the parameter is set to 0.
\end{itemize}
You can also set the \verb|Seed| of the simulation, 
a parameter 
\verb|EpsGen| defining the randomness in cold gauge conf generation, and the 
verbosity level \verb|VerbosityLv|. 


    \item{Measures: \verb|GaugeMeasuresSettings|.} Now, only the name of the 
        file for the {\bf basic} gauge measures is specified (plaquette, 
rectangle, real and imaginary part of the Polyakov loop). 
        Notice that the name of the 
output file will be dependent on the relevant parameters of the input file, 
through a simple hash procedure (see \verb|src/Include/hash.c|.)
        Of course there are, in principle, a lot of observables that 
can be implemented. Now only the Polyakov loop and the topological charge are 
implemented but the topological charge measurement (which involves cooling) is 
not currently handled in the input file. 
    \item{Measures: \verb|FermionMeasuresSettings|.}
    Apart from the file name (which will be modified as in the gauge case),   
you can set how many random vectors will be used for the measurement of various 
fermionic quantities. For the chiral condensate and the quark number only one 
inversion for random vector is needed. For the susceptiiblities, two inversions 
per random vector are needed. The inversion in the chiral case and the inversion 
in the quark number case have to be performed on different vectors. Notice: the 
number of ``single inversions'' must be higher than the 
number of ``double inversions''; this is because each ``double inversion'' 
contains a ``single inversion'', so it is assumed that there are no reasons to 
ask for \verb|SingleInvNVectors| to be smaller than 
\verb|DoubleInvNVectorsChiral| or \verb|DoubleInvNVectorsQuarkNumber|.
    
    
    \item{Device: \verb|DeviceSettings|.}  Here you can set the number of 
mpi ranks (one or more), which must be equal to  the one set at compilation 
time. Rank 0 (or the whole program, in the case of a single process) will run 
on the device speficied by \verb|device_choice|. The other ranks, will chose 
different gpus according to the scheme
\begin{verbatim}
 d = 
\end{verbatim}


\textbf{Notice: } set this to 0 when using slurm as a job scheduler. In any 
case, ask the sysadmin about this.\\
When running on multiple 
    \item{\verb|Geometry|} Here the lattice dimensions are listed and the 
        direction mapping too. Unfortunately, the lattice dimensions cannot be set in 
        this way; as in the case of \verb|StoutRho| they are read just to check the 
        intention of the user and suggest recompilation. \\
        Note that only one dimension can be parallelized, and for performance 
reasons it must be the one relative to the most extern index in the arrays, 
that is the fourth one. With the ``map'' parameters, you can choose which 
physical directions correspond to each logical direction. By default, the 
       
        
        
   \item{Algorithmic optimizations: \verb|InverterTicks|. }
   These actually refer to the single shift inverter: very little is doable for 
the multishift one.
   \begin{itemize}
    \item \verb|singlePInvAccelMultiInv| : don't touch this, multishift 
inverter is still faster (keep 0 or delete).
 \item \verb|useMixedPrecision|: in single inversions, this should provide a 
substantial boost. Default is 0, by now.
 \item \verb|restartingEvery| : restarting of the inversion algorithm takes 
place every how-many-you-decide steps. Default 10000.
 \item \verb|mixedPrecisionDelta| : in the mixed precision CG algorithm, the 
residue must be calculated in double precision at some points. In practice, it 
is recalculated in double precision when its magnitude varies of a factor, 
which is specified here. Default : 0.1.

   \end{itemize}

   
   
        
        
    \item{Debug: \verb|DebugSettings|.}
    \begin{itemize}
    \item \verb|UseILDG|: if this flag is set to $1$, the gauge configuration 
file  I/O will be done using ILDG format. The default value is 1, so ILDG 
activated.
    \item \verb|SaveAllAtEnd|: In case you want to reproduce a bug, you want to 
start from the same gauge configuration and from the same state of the random 
number generator(s). Setting this option to 0 will make the program not 
overwrite the gauge configuration file and the random generator status file(s). 
The default value is 1, of course.
    \item 
\end{itemize}
\end{itemize}


\subsubsection{Remarks: verbosity level}
Don't forget to set the \verb|VerbosityLv| variable to the desired result. 
The value of it is assigned to the \verb|verbosity_lv| global variable.
As a rule of thumb, '5' prints all, '1' prints nearly nothing. Note
that the behaviour of \verb|VerbosityLv| is likely to change over time, no
policy is yet decided. If you implement anything new, you can use this 
feature to give informative messages.

\subsubsection{Remarks: Optimization of parameters}

There is an interplay between \verb|NmdSteps|, \verb|GaugeSubSteps|,
\verb|residue_md| and the order of the rational approximations used in the 
molecular dynamics. The goal of the optimization is to obtain a reasonable 
value of the acceptance from the Metropolis test while using the lowest value 
possible. For a discussion on this topic, see Sec.(\ref{tuning}).


\subsubsection{Remarks: seemingly insignificant things that you have to take 
care of}
A parameter named \verb|ExpMaxEigenvalue| can be used in the 
\verb|MontecarloParameters| group, and represent an estimate
of the maximum eigenvalue of the Dirac Operator. Its default value is $5.5$.  
This will influence  the choice of the rational approximations you need. 
Notice that the highest eigenvalue of the fermion matrix is calculated during 
the update and printed out if the right verbosity value is set. Notice that the 
smaller \verb|StoutSteps is|, the larger this number is likely to be. 

What you \emph{can't} set with the input file (now) and you have to
recompile\footnote{ It is not a problem if you use gcc or pgi to compile for 
    CPUs, but compiling with pgi for GPUs can require time, depending on the
version of the compiler.}:
\begin{itemize}
    \item The type of gauge action. [triggers some long recompilation]
    \item The stouting strength RHO 
    \item The lattice dimensions in /src/OpenAcc/geometry.h [this will
        practically trigger a full recompilation].
\end{itemize}
Of course, changing the input file format requires to recompile the functions 
to read it, in \verb|/src/Include/init.c|.

\subsection{ Rational Approximation files}
Rational approximations are used to approximate powers (like $1/4$,
$1/8$) of the Dirac Operator.
You need $3$ rational approximations files for each quark flavour in the
theory, one for the first inversion (to map the original
gaussianly generated pseudofermion to one generated according to the
right pdf), one used during the molecular dynamics evolution (with a
lower order, so less precise, for speed), and a third one for the
calculation of the final action for the metropolis test (more
precise). At the time of writing, it is common that at least 2 flavour are 
degenerate in mass, and this allows you to use the same set of rational
approximations. For example, for a $N_f=2+1$ simulation, you need a total
of 6 rational approximations.
NOTICE: the name of the rational approximation file is related to
its content, and can be generated using the function
\verb|char* rational_approx_filename()| in
\verb|src/RationalApprox/rationalapprox.h|.

\subsubsection{ How to generate and check rational approximations}
Use the tools in \verb|tools/|. A makefile should be there. 
\begin{itemize}
    \item '\verb|rgen|' (\verb|rationalapprox_calc.c|) 
        the tool for generating them, it takes the tolerance (plus other
        things) as input and will try to increase the number of poles until
        the required level of acuracy is reached. It takes time to generate
        high order approximations (~minutes). Notice that it needs arbitary
        precision libraries to be compiled.
    \item '\verb|rgenfo|' (\verb|rationalapprox_calc.c|) 
        Same as rgen, but the order of the approximation is fixed.
    \item '\verb|ratnormalizer|' (\verb|normalize_ratapprox.c|) 
        takes a rational approx and
        rescales it so that the upper limit of the validity range is 1.
    \item '\verb|eval_ratapprox|' (\verb|eval_ratapprox.c|)
        Are you sure that your rational approx works? With this tool, you
        can check! \verb|bc -l| is your friend too.
\end{itemize}

\subsection{Gauge configuration files}
For the format of the gauge configuration files, look in 
\verb|src/OpenAcc/io.c: print_su3_soa() - read_su3_soa()| or similar.
Notice there are two possible formats for the gauge configuration files: the 
ASCII format, and the ILDG format. The first one is simple and is useful for 
simplicity. The ILDG file format is a standard format used in Lattice QCD, 
where the data is stored in binary format (notice: big endian).  
The read and/or write of the initial/final gauge configuration file
is usually done in \verb|/src/OpenAcc/main.c|.


\section{Debug Tools}
Given the intended usage of debug tools, the user/developer should be always 
wary of them, and check that they work as he would expect: debug tools are not 
tested as extensively as the production code. None the less, it seems pretty
reasonable to add some work that has been done on this tools in the released 
code.
\subsection{ NORANDOM Mode}
If you need your run to be reproducible for a whole molecular dynamics 
trajectory, you don't trust the random number generatorand don't want to use 
it, you can use
\verb|#define NORANDOM|. It may also be useful to shorten the length of the 
trajectory (to improve acceptance).
At present random number generation is used in:
\begin{itemize}
    \item The generation of a new configuration;
    \item The generation of the pseudo fermions for the molecular dynamics 
        evolution;
    \item The generation of the momenta for the molecular dynamics evolution.
    \item It is also used to generate the pseudofermions in stochastic fermion 
        measurements, however at present this aspect is not touched by 
        \verb|#define NORANDOM|.
\end{itemize}
The only files where \verb|#define NORANDOM| has an effect are, at present, 
\verb|/src/OpenAcc/main.c| and \verb|/src/OpenAcc/update_versatile.c|. In order 
to activate NORANDOM mode, you can also add a suitable \verb|-DNORANDOM| in the 
\verb|COMPILER_FLAGS| in the makefile (don't forget to also ``touch'' 
\verb|main.c| and \verb|update_versatile.c| to trigger the compilation of those 
files with \verb|make|).



\subsection{The DbgTools directory}
In this directory you can find various \emph{skeletons} for many testing needs:
\begin{itemize}
    \item Tests and benchmarks for $D_{eo}$ and $D_{oe}$, the Dirac 
        operator (\verb|deo_doe_test.c|), in single an double precision.
    \item Test and benchmarks for the multishift inverter in single and double 
precision.
    \item Tests for some functions involved in stouting
    \item (to be continued)
\end{itemize}
In the header file \verb|dbgtools.h| you can find the declarations of many 
functions that print out or read from file various datatypes.


\section{Tuning}
\label{tuning}
Holy text report that optimal hybrid montecarlo acceptance should be 87.5\% or 
so. 
The probability for a new configuration to be accepted is related to the action 
difference between the start and the end of the trajectory, which should be zero 
in 
perfect conditions. The reasons why it \emph{will} be different from zero are:
\begin{itemize}
    \item A mismatch between the 3 rational approximations used. A fast test could 
        be done 
        to check if the first one and the last one are in good agreement, just removing 
        all 
        molecular dynamics in between\footnote{At the moment, this result is NOT 
            achieved by 
        putting the relevant parameters in the input file to zero.}. 
    \item Errors in the integration of the trajectory.
\end{itemize}
As far as the first point is concerned: since we want our results to be 
accurate, we 
would like the last inversion to be as 
``exact'' as possible, while we can save some computing time on the molecular 
dynamics 
inversions, which will be performed many times and whose precision (as long as 
the last 
action calculation is ``exact'') can be traded for speed. The first inversion 
can be 
done precisely since it will be performed only once. Optimization can thus be 
made with
the choice of the rational approximations used during molecular dynamics. There 
is a 
single parameter which must be tuned, which is the maximum error of the 
approximation. 
Reducing the error will determine an increase in the order of the approximation 
and a 
reduction in the target residue in the inversions. The higher the order, the 
slower; 
the smaller the target residue, the slower. \\
As far as the integration errors are concerned, the most basic observation is 
that, for 
the Omelyan 2nd order integrator, the action difference (which is an error) 
should go 
like $[t/n_{md}]^2$, where $n_{md}$ is the number of steps in the molecular 
dynamics 
trajectory, while $t$ is the molecular dynamics time\footnote{(Note for self) 
this topic has been already discussed in 
    the 
literature, check}. While there is no way a priori to fix the parameters apart 
from 
experience (read: look to another successful run), once a choice of the 
parameters is 
reached for which acceptance is satisfying, there are some possible rules of 
thumb that 
may help in optimizing $n_{md}$ and the number of gauge substeps $n_{gss}$:
\begin{itemize}
    \item The norm of the fermion force times the number of gauge substeps should 
        be 
        roughly equal to the norm of the gauge force. The value of both norms is 
        calculated by 
        the program and printed when the right level of verbosity is chosen;
    \item The norm of the variation of the fermion force every half Omelyan macro 
        step 
        (which is $n_{gss}$ times a micro step), and the norm of the variation of the 
        gauge 
        force every Omelyan micro step should be equal. These quantities are also 
        printed at 
        the same level of verbosity.
    \item (Not implemented) The same reasoning could be done with the maximum values 
        found.
\end{itemize}









\section{Input file reading (and advices for format modifications).}
Input file reading is done in \verb|/src/Include/init.c|. The idea here is to 
read parameters from a file and store them into global variables (or, better, 
into variables of limited number of global structures). The input file should 
be composed by parameter ``macro groups'', with a name that is needed to find 
them, and each of the parameter macro groups consist of a collection of tags 
which are put next to a value (an integer, a floating point number, or a 
string). The function 
\verb|set_global_vars_and_fermions_from_input_file()| does the 
following things:
\begin{enumerate}
    \item Tries to open the specified input file. If it is successful:
        \begin{enumerate}
            \item Loads all the file as an array of lines, and modifies lines removing 
                comments (everything beyond a \verb|#| sign, including \verb|#| itself)
            \item Scan the file to find names of macro-groups of parameters, thus 
                identifying for each group a type, an initial line and an ending line;
            \item For each of the groups found, it calls the appropriate parser (which 
                is actually a wrapper) passing to it the correct \emph{global} data member, 
                the start file line and the end file line of the group. Every parser-wrapper 
                defines an array of structures of the kind \verb|par_info|:
                \begin{verbatim}
                typedef struct par_info_t{
                    void* par;
                    int type; // 0 = int, 1 = double, 2 = string
                    char* name;
                    int is_optional;
                    const void * default_value;
                }par_info;
                \end{verbatim}
                Inside every parser-wrapper, this array is passed to a real parser that looks 
                for the \verb|name| in the range of lines specified by the parser-wrapper, 
                reads a value whose type is speficied in \verb|type|, and puts it into the 
                variable pointed at by \verb|*par|. If no value specified by \verb|type| is 
                found, and if \verb|is_optional| is zero, an error will be generated; if 
                instead \verb|is_optional| is one, the default value read from the pointer 
                \verb|*default_value| will be assigned to the variable pointer at by 
                \verb|*par|.
        \end{enumerate}
    \item If it is not opened for any reason, it goes into ``help'' mode, and the 
        same chain of parser-wrappers and parser is called in help mode writing an 
        \verb|input_template| file.
\end{enumerate}
If you want to add another parameter ``macro group'', you have to add the name 
to the \verb|par_macro_groups_names| list, increasing also \verb|NPMGTYPES|.
You also need to create the correct parser-wrapper for it, and modify the 
\verb|set_global_vars_and_fermions_from_input_file()| function to accomodate 
the reading of that parameter ``macro group''.\\
If you want just to add a parameter to an existing group, you just have to 
modify its parser-wrapper and possibly the structure that should hold the value 
of that new parameter.

\section{Configuration file reading}
All this is performed in \verb|src/OpenAcc/io.c|.

\subsection{ASCII Format}
The ASCII format is really useful for checking and debugging, and at present is 
the only one which is extensively tested. This format is fast to modif (of 
course you need to modify both reading and writing functions), and being able 
to edit the configuration files with a text editor might be useful.
Unfortunately files tend to be files which are 3 times larger than binary 
files, and for large lattices this might be a problem.

\subsection{ILDG format (binary)}
By activating the correct flag in the input file, the ILDG format will be read 
and written instead. The ILDG format is used for gauge configuration 
storage which is a diffused standard (Google it). It is made up by LIME records 
(google LIME, scidac, there is also a GitHub repo with documentation), which 
are made up of a header and a body. Binary data in the header and the body are 
assumed to be Big Endian. The header has a ``type'' string, which identifies 
the 
kind of record.
The standard ILDG format requires a \texttt{ildg-format} record, an 
\texttt{ildg-binary-data} record, and a \texttt{ildg-data-lfn} record:
\begin{itemize}
    \item The \texttt{ildg-format} record contains, most notably the dimensions of 
        the configuration, the precision and the kind of gauge field (in ASCII);
    \item \texttt{ildg-binary-data} contains instead the values of the 
        configuration gauge links in binary format;
    \item The \texttt{ildg-data-lfn} is not actually used here.
\end{itemize}
Non standard records:
\begin{itemize}
    \item \verb|MD_traj|: a number identifying the Montecarlo Trajectory, needed 
        to restart the simulation (ASCII format).
    \item \verb|input-file|: the entire input file with comments (ASCII format).
\end{itemize}
You can look into \verb|src/OpenAcc/io.c| or into a gauge configuration 
file with a text editor to get more informations.

\subsection{Configuration file format conversion}
A tool for file conversion is \verb|/src/DbgTools/ildgtest.c|. You can add it 
to the list of arguments you pass to \verb|generate_makefile.py| in order to 
build a makefile that will have, amongst other targets,\verb|ildgtest|.
Modify the \verb|main()| to obtain your desired result.

